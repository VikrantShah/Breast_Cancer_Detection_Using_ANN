{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 2:-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'M' 'M'\n",
      " 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'M' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'B']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAP9klEQVR4nO3df6yeZX3H8feHgkCmRkgPrLRl7UiNK0xLdtI5/WNM3GAuW9EMUxJds5GUPyDRxC0B/5g408xkoDGbmNSJVuNkzdTRGd2Gjc6YOeqBVKBAYyMMDu3oEXWCMV1av/vj3L14aE/LU+B+nsM571dy57nv676u+/mepOkn1/3rSVUhSRLAaeMuQJI0fxgKkqTGUJAkNYaCJKkxFCRJzenjLuDFWLp0aa1atWrcZUjSy8o999zzw6qamGvfyzoUVq1axdTU1LjLkKSXlST/faJ9nj6SJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNS/rJ5qlheyxv/r1cZegeejCv7y/1+P3NlNIclaSXUm+l2RPkg927TcneSLJ7m5528CYm5LsS7I3yRV91SZJmlufM4VDwFuq6pkkZwDfTvK1bt9Hq+qWwc5J1gIbgYuBC4CvJ3ltVR3psUZJ0oDeZgo165lu84xuOdkPQm8A7qiqQ1X1CLAPWN9XfZKk4/V6oTnJkiS7gYPAXVV1d7frhiT3Jbk9yTld23Lg8YHh013bscfcnGQqydTMzEyf5UvSotNrKFTVkapaB6wA1ie5BPgEcBGwDjgA3Np1z1yHmOOYW6tqsqomJybmfB24JOkFGsktqVX1E+CbwJVV9WQXFr8APsmzp4imgZUDw1YA+0dRnyRpVp93H00keU23fjbwVuDhJMsGur0deKBb3wFsTHJmktXAGmBXX/VJko7X591Hy4BtSZYwGz7bq+orST6XZB2zp4YeBa4DqKo9SbYDDwKHgeu980iSRqu3UKiq+4BL52h/90nGbAG29FWTJOnkfM2FJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUtNbKCQ5K8muJN9LsifJB7v2c5PcleT73ec5A2NuSrIvyd4kV/RVmyRpbn3OFA4Bb6mqNwDrgCuTvBG4EdhZVWuAnd02SdYCG4GLgSuB25Is6bE+SdIxeguFmvVMt3lGtxSwAdjWtW8DrurWNwB3VNWhqnoE2Aes76s+SdLxer2mkGRJkt3AQeCuqrobOL+qDgB0n+d13ZcDjw8Mn+7ajj3m5iRTSaZmZmb6LF+SFp1eQ6GqjlTVOmAFsD7JJSfpnrkOMccxt1bVZFVNTkxMvFSlSpIY0d1HVfUT4JvMXit4MskygO7zYNdtGlg5MGwFsH8U9UmSZvV599FEktd062cDbwUeBnYAm7pum4A7u/UdwMYkZyZZDawBdvVVnyTpeKf3eOxlwLbuDqLTgO1V9ZUk3wG2J7kWeAy4GqCq9iTZDjwIHAaur6ojPdYnSTpGb6FQVfcBl87R/hRw+QnGbAG29FWTJOnkfKJZktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqektFJKsTPKNJA8l2ZPkPV37zUmeSLK7W942MOamJPuS7E1yRV+1SZLmdnqPxz4MvK+q7k3yKuCeJHd1+z5aVbcMdk6yFtgIXAxcAHw9yWur6kiPNUqSBvQ2U6iqA1V1b7f+NPAQsPwkQzYAd1TVoap6BNgHrO+rPknS8UZyTSHJKuBS4O6u6YYk9yW5Pck5Xdty4PGBYdPMESJJNieZSjI1MzPTY9WStPj0HgpJXgl8EXhvVf0U+ARwEbAOOADcerTrHMPruIaqrVU1WVWTExMTPVUtSYtTr6GQ5AxmA+HzVfUlgKp6sqqOVNUvgE/y7CmiaWDlwPAVwP4+65MkPVefdx8F+BTwUFV9ZKB92UC3twMPdOs7gI1JzkyyGlgD7OqrPknS8fq8++jNwLuB+5Ps7treD1yTZB2zp4YeBa4DqKo9SbYDDzJ759L13nkkSaPVWyhU1beZ+zrBV08yZguwpa+aJEkn5xPNkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktT0+ctrLwu/8RefHXcJmofu+Zs/GXcJ0lg4U5AkNYaCJKkZKhSS7BymTZL08nbSUEhyVpJzgaVJzklybresAi54nrErk3wjyUNJ9iR5T9d+bpK7kny/+zxnYMxNSfYl2Zvkihf/50mSTsXzzRSuA+4BXtd9Hl3uBD7+PGMPA++rql8D3ghcn2QtcCOws6rWADu7bbp9G4GLgSuB25IseSF/lCTphTlpKFTVx6pqNfDnVfWrVbW6W95QVX/3PGMPVNW93frTwEPAcmADsK3rtg24qlvfANxRVYeq6hFgH7D+Bf9lkqRTNtQtqVX1t0neBKwaHFNVQ93P2Z1uuhS4Gzi/qg504w8kOa/rthz4r4Fh013bscfaDGwGuPDCC4f5eknSkIYKhSSfAy4CdgNHuuYCnjcUkrwS+CLw3qr6aZITdp2jrY5rqNoKbAWYnJw8br8k6YUb9uG1SWBtVZ3Sf8JJzmA2ED5fVV/qmp9MsqybJSwDDnbt08DKgeErgP2n8n2SpBdn2OcUHgB++VQOnNkpwaeAh6rqIwO7dgCbuvVNzF60Ptq+McmZSVYDa4Bdp/KdkqQXZ9iZwlLgwSS7gENHG6vqj04y5s3Au4H7k+zu2t4PfBjYnuRa4DHg6u5Ye5JsBx5k9s6l66vqyPGHlST1ZdhQuPlUD1xV32bu6wQAl59gzBZgy6l+lyTppTHs3Uf/0XchkqTxG/buo6d59k6gVwBnAD+rqlf3VZgkafSGnSm8anA7yVX4YJkkLTgv6C2pVfXPwFte4lokSWM27Omjdwxsnsbscws+OCZJC8ywdx/94cD6YeBRZt9VJElaQIa9pvCnfRciSRq/YX9kZ0WSLyc5mOTJJF9MsqLv4iRJozXsheZPM/saiguYfXPpv3RtkqQFZNhQmKiqT1fV4W75DDDRY12SpDEYNhR+mORdSZZ0y7uAp/osTJI0esOGwp8B7wT+BzgA/DHgxWdJWmCGvSX1Q8CmqvoxQJJzgVuYDQtJ0gIx7Ezh9UcDAaCqfsTsz2tKkhaQYUPhtCTnHN3oZgrDzjIkSS8Tw/7Hfivwn0n+idnXW7wTf/dAkhacYZ9o/mySKWZfghfgHVX1YK+VSZJGbuhTQF0IGASStIC9oFdnS5IWJkNBktT0FgpJbu9eoPfAQNvNSZ5Isrtb3jaw76Yk+5LsTXJFX3VJkk6sz5nCZ4Ar52j/aFWt65avAiRZC2wELu7G3JZkSY+1SZLm0FsoVNW3gB8N2X0DcEdVHaqqR4B9+BvQkjRy47imcEOS+7rTS0cfiFsOPD7QZ7prO06SzUmmkkzNzMz0XaskLSqjDoVPABcB65h9sd6tXXvm6Dvnb0BX1daqmqyqyYkJ394tSS+lkYZCVT1ZVUeq6hfAJ3n2FNE0sHKg6wpg/yhrkySNOBSSLBvYfDtw9M6kHcDGJGcmWQ2sAXaNsjZJUo8vtUvyBeAyYGmSaeADwGVJ1jF7auhR4DqAqtqTZDuzT0wfBq6vqiN91SZJmltvoVBV18zR/KmT9N+CL9mTpLHyiWZJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkprdQSHJ7koNJHhhoOzfJXUm+332eM7DvpiT7kuxNckVfdUmSTqzPmcJngCuPabsR2FlVa4Cd3TZJ1gIbgYu7MbclWdJjbZKkOfQWClX1LeBHxzRvALZ169uAqwba76iqQ1X1CLAPWN9XbZKkuY36msL5VXUAoPs8r2tfDjw+0G+6aztOks1JppJMzczM9FqsJC028+VCc+Zoq7k6VtXWqpqsqsmJiYmey5KkxWXUofBkkmUA3efBrn0aWDnQbwWwf8S1SdKiN+pQ2AFs6tY3AXcOtG9McmaS1cAaYNeIa5OkRe/0vg6c5AvAZcDSJNPAB4APA9uTXAs8BlwNUFV7kmwHHgQOA9dX1ZG+apMkza23UKiqa06w6/IT9N8CbOmrHknS85svF5olSfOAoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkprTx/GlSR4FngaOAIerajLJucA/AquAR4F3VtWPx1GfJC1W45wp/E5VrauqyW77RmBnVa0BdnbbkqQRmk+njzYA27r1bcBVY6xFkhalcYVCAf+e5J4km7u286vqAED3ed5cA5NsTjKVZGpmZmZE5UrS4jCWawrAm6tqf5LzgLuSPDzswKraCmwFmJycrL4KlKTFaCwzhara330eBL4MrAeeTLIMoPs8OI7aJGkxG3koJPmlJK86ug78HvAAsAPY1HXbBNw56tokabEbx+mj84EvJzn6/f9QVf+a5LvA9iTXAo8BV4+hNkla1EYeClX1A+ANc7Q/BVw+6nokSc+aT7ekSpLGzFCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEnNvAuFJFcm2ZtkX5Ibx12PJC0m8yoUkiwBPg78PrAWuCbJ2vFWJUmLx7wKBWA9sK+qflBV/wfcAWwYc02StGicPu4CjrEceHxgexr4zcEOSTYDm7vNZ5LsHVFti8FS4IfjLmI+yC2bxl2Cnst/m0d9IC/FUX7lRDvmWyjM9dfWczaqtgJbR1PO4pJkqqomx12HdCz/bY7OfDt9NA2sHNheAewfUy2StOjMt1D4LrAmyeokrwA2AjvGXJMkLRrz6vRRVR1OcgPwb8AS4Paq2jPmshYTT8tpvvLf5oikqp6/lyRpUZhvp48kSWNkKEiSGkNhkUtSST43sH16kpkkXxlnXRJAkiNJdif5XpJ7k7xp3DUtdPPqQrPG4mfAJUnOrqqfA78LPDHmmqSjfl5V6wCSXAH8NfDb4y1pYXOmIICvAX/QrV8DfGGMtUgn8mrgx+MuYqEzFASz75jamOQs4PXA3WOuRzrq7O700cPA3wMfGndBC52nj0RV3ZdkFbOzhK+OtxrpOQZPH/0W8Nkkl5T30vfGmYKO2gHcgqeONE9V1XeYfTHexLhrWcicKeio24H/rar7k1w27mKkYyV5HbNvOnhq3LUsZIaCAKiqaeBj465DOsbZSXZ36wE2VdWRcRa00PmaC0lS4zUFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSc3/A9uyJPdBp2WVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y, label = \"Diagnosis\")\n",
    "plt.savefig(\"Diagnosis.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.350e+01 1.271e+01 8.569e+01 ... 2.210e-02 2.267e-01 6.192e-02]\n",
      " [8.196e+00 1.684e+01 5.171e+01 ... 2.564e-02 3.105e-01 7.409e-02]\n",
      " [1.719e+01 2.207e+01 1.116e+02 ... 1.984e-01 3.216e-01 7.570e-02]\n",
      " ...\n",
      " [9.436e+00 1.832e+01 5.982e+01 ... 5.052e-02 2.454e-01 8.136e-02]\n",
      " [9.720e+00 1.822e+01 6.073e+01 ... 0.000e+00 1.909e-01 6.559e-02]\n",
      " [1.151e+01 2.393e+01 7.452e+01 ... 9.653e-02 2.112e-01 8.732e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.340e+01 2.052e+01 8.864e+01 ... 2.051e-01 3.585e-01 1.109e-01]\n",
      " [1.321e+01 2.525e+01 8.410e+01 ... 6.005e-02 2.444e-01 6.788e-02]\n",
      " [1.402e+01 1.566e+01 8.959e+01 ... 8.216e-02 2.136e-01 6.710e-02]\n",
      " ...\n",
      " [1.243e+01 1.700e+01 7.860e+01 ... 2.832e-02 1.901e-01 5.932e-02]\n",
      " [1.603e+01 1.551e+01 1.058e+02 ... 1.981e-01 3.019e-01 9.124e-02]\n",
      " [1.232e+01 1.239e+01 7.885e+01 ... 9.391e-02 2.827e-01 6.771e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1725321  -1.54666427 -0.25304409 ... -1.41901523 -1.01864958\n",
      "  -1.21862477]\n",
      " [-1.67708411 -0.55788745 -1.65424438 ... -1.3647525   0.32299179\n",
      "  -0.54226125]\n",
      " [ 0.87418677  0.69424397  0.81538145 ...  1.28339165  0.50070324\n",
      "  -0.45278342]\n",
      " ...\n",
      " [-1.32534118 -0.20355581 -1.31982018 ... -0.9833805  -0.71926183\n",
      "  -0.13822158]\n",
      " [-1.2447807  -0.22749713 -1.28229539 ... -1.75777412 -1.59180903\n",
      "  -1.01465976]\n",
      " [-0.73702277  1.13955266 -0.71365054 ... -0.27811821 -1.26680521\n",
      "   0.19301314]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20089847  0.32315339 -0.1313978  ...  1.38609231  1.09147372\n",
      "   1.50350219]\n",
      " [-0.25479456  1.45557818 -0.31860937 ... -0.83730031 -0.73527187\n",
      "  -0.88739005]\n",
      " [-0.025027   -0.84039511 -0.09222357 ... -0.49838813 -1.22838111\n",
      "  -0.93073956]\n",
      " ...\n",
      " [-0.47605221 -0.51958133 -0.54540754 ... -1.32367223 -1.60461706\n",
      "  -1.36312314]\n",
      " [ 0.54513693 -0.8763071   0.57621247 ...  1.27879312  0.18530544\n",
      "   0.41087222]\n",
      " [-0.50725521 -1.62327652 -0.53509853 ... -0.31827877 -0.12208733\n",
      "  -0.89683802]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer, first hidden layer and dropout layer\n",
    "classifier.add(Dense(16, activation = \"relu\"))\n",
    "classifier.add(Dropout(rate = 1.0))\n",
    "\n",
    "# Adding the second hidden layer and dropout layer\n",
    "classifier.add(Dense(32, activation = \"relu\"))\n",
    "classifier.add(Dropout(rate = 1.0))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "483/483 [==============================] - 0s 514us/step - loss: 0.7720 - accuracy: 0.3623\n",
      "Epoch 2/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.7014 - accuracy: 0.3789\n",
      "Epoch 3/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.6375 - accuracy: 0.4555\n",
      "Epoch 4/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.5797 - accuracy: 0.6253\n",
      "Epoch 5/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.5295 - accuracy: 0.8323\n",
      "Epoch 6/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.4806 - accuracy: 0.9110\n",
      "Epoch 7/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.4357 - accuracy: 0.9400\n",
      "Epoch 8/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.3919 - accuracy: 0.9503\n",
      "Epoch 9/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.3506 - accuracy: 0.9524\n",
      "Epoch 10/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.3128 - accuracy: 0.9545\n",
      "Epoch 11/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.2781 - accuracy: 0.9586\n",
      "Epoch 12/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.2473 - accuracy: 0.9586\n",
      "Epoch 13/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.2199 - accuracy: 0.9607\n",
      "Epoch 14/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1967 - accuracy: 0.9607\n",
      "Epoch 15/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1772 - accuracy: 0.9607\n",
      "Epoch 16/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1611 - accuracy: 0.9607\n",
      "Epoch 17/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.1479 - accuracy: 0.9607\n",
      "Epoch 18/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1364 - accuracy: 0.9607\n",
      "Epoch 19/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1276 - accuracy: 0.9648\n",
      "Epoch 20/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.1196 - accuracy: 0.9669\n",
      "Epoch 21/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.1134 - accuracy: 0.9689\n",
      "Epoch 22/125\n",
      "483/483 [==============================] - 0s 25us/step - loss: 0.1078 - accuracy: 0.9689\n",
      "Epoch 23/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1034 - accuracy: 0.9689\n",
      "Epoch 24/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0991 - accuracy: 0.9710\n",
      "Epoch 25/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0958 - accuracy: 0.9752\n",
      "Epoch 26/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0924 - accuracy: 0.9752\n",
      "Epoch 27/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0900 - accuracy: 0.9731\n",
      "Epoch 28/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0875 - accuracy: 0.9752\n",
      "Epoch 29/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0851 - accuracy: 0.9752\n",
      "Epoch 30/125\n",
      "483/483 [==============================] - 0s 31us/step - loss: 0.0831 - accuracy: 0.9752\n",
      "Epoch 31/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0814 - accuracy: 0.9752\n",
      "Epoch 32/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0796 - accuracy: 0.9752\n",
      "Epoch 33/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0780 - accuracy: 0.9752\n",
      "Epoch 34/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0767 - accuracy: 0.9752\n",
      "Epoch 35/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0753 - accuracy: 0.9752\n",
      "Epoch 36/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0741 - accuracy: 0.9752\n",
      "Epoch 37/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0728 - accuracy: 0.9752\n",
      "Epoch 38/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0717 - accuracy: 0.9752\n",
      "Epoch 39/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0706 - accuracy: 0.9752\n",
      "Epoch 40/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0696 - accuracy: 0.9752\n",
      "Epoch 41/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0687 - accuracy: 0.9752\n",
      "Epoch 42/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0677 - accuracy: 0.9772\n",
      "Epoch 43/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0668 - accuracy: 0.9772\n",
      "Epoch 44/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0660 - accuracy: 0.9772\n",
      "Epoch 45/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0653 - accuracy: 0.9772\n",
      "Epoch 46/125\n",
      "483/483 [==============================] - 0s 25us/step - loss: 0.0644 - accuracy: 0.9772\n",
      "Epoch 47/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0638 - accuracy: 0.9772\n",
      "Epoch 48/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0631 - accuracy: 0.9793\n",
      "Epoch 49/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0623 - accuracy: 0.9793\n",
      "Epoch 50/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0616 - accuracy: 0.9793\n",
      "Epoch 51/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0610 - accuracy: 0.9793\n",
      "Epoch 52/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0603 - accuracy: 0.9793\n",
      "Epoch 53/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0597 - accuracy: 0.9793\n",
      "Epoch 54/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0591 - accuracy: 0.9793\n",
      "Epoch 55/125\n",
      "483/483 [==============================] - 0s 18us/step - loss: 0.0585 - accuracy: 0.9793\n",
      "Epoch 56/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0580 - accuracy: 0.9793\n",
      "Epoch 57/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0573 - accuracy: 0.9793\n",
      "Epoch 58/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0568 - accuracy: 0.9793\n",
      "Epoch 59/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0562 - accuracy: 0.9793\n",
      "Epoch 60/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0556 - accuracy: 0.9793\n",
      "Epoch 61/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0551 - accuracy: 0.9793\n",
      "Epoch 62/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0546 - accuracy: 0.9793\n",
      "Epoch 63/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0540 - accuracy: 0.9793\n",
      "Epoch 64/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0534 - accuracy: 0.9814\n",
      "Epoch 65/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0529 - accuracy: 0.9814\n",
      "Epoch 66/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0524 - accuracy: 0.9814\n",
      "Epoch 67/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0519 - accuracy: 0.9814\n",
      "Epoch 68/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0514 - accuracy: 0.9814\n",
      "Epoch 69/125\n",
      "483/483 [==============================] - 0s 27us/step - loss: 0.0509 - accuracy: 0.9814\n",
      "Epoch 70/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0505 - accuracy: 0.9793\n",
      "Epoch 71/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0500 - accuracy: 0.9814\n",
      "Epoch 72/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0495 - accuracy: 0.9834\n",
      "Epoch 73/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0490 - accuracy: 0.9834\n",
      "Epoch 74/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0486 - accuracy: 0.9834\n",
      "Epoch 75/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0481 - accuracy: 0.9834\n",
      "Epoch 76/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0476 - accuracy: 0.9834\n",
      "Epoch 77/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0473 - accuracy: 0.9834\n",
      "Epoch 78/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0467 - accuracy: 0.9834\n",
      "Epoch 79/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0463 - accuracy: 0.9834\n",
      "Epoch 80/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 0s 23us/step - loss: 0.0459 - accuracy: 0.9834\n",
      "Epoch 81/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0455 - accuracy: 0.9834\n",
      "Epoch 82/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0450 - accuracy: 0.9834\n",
      "Epoch 83/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0446 - accuracy: 0.9834\n",
      "Epoch 84/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0442 - accuracy: 0.9834\n",
      "Epoch 85/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0437 - accuracy: 0.9855\n",
      "Epoch 86/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0434 - accuracy: 0.9855\n",
      "Epoch 87/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0428 - accuracy: 0.9855\n",
      "Epoch 88/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0425 - accuracy: 0.9876\n",
      "Epoch 89/125\n",
      "483/483 [==============================] - 0s 25us/step - loss: 0.0420 - accuracy: 0.9876\n",
      "Epoch 90/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0416 - accuracy: 0.9876\n",
      "Epoch 91/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0412 - accuracy: 0.9876\n",
      "Epoch 92/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0408 - accuracy: 0.9876\n",
      "Epoch 93/125\n",
      "483/483 [==============================] - 0s 27us/step - loss: 0.0404 - accuracy: 0.9876\n",
      "Epoch 94/125\n",
      "483/483 [==============================] - 0s 29us/step - loss: 0.0400 - accuracy: 0.9896\n",
      "Epoch 95/125\n",
      "483/483 [==============================] - 0s 31us/step - loss: 0.0396 - accuracy: 0.9896\n",
      "Epoch 96/125\n",
      "483/483 [==============================] - 0s 27us/step - loss: 0.0391 - accuracy: 0.9917\n",
      "Epoch 97/125\n",
      "483/483 [==============================] - 0s 29us/step - loss: 0.0387 - accuracy: 0.9917\n",
      "Epoch 98/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0383 - accuracy: 0.9917\n",
      "Epoch 99/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0379 - accuracy: 0.9917\n",
      "Epoch 100/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0376 - accuracy: 0.9917\n",
      "Epoch 101/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0371 - accuracy: 0.9917\n",
      "Epoch 102/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0367 - accuracy: 0.9917\n",
      "Epoch 103/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0364 - accuracy: 0.9917\n",
      "Epoch 104/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0360 - accuracy: 0.9917\n",
      "Epoch 105/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0355 - accuracy: 0.9917\n",
      "Epoch 106/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0351 - accuracy: 0.9917\n",
      "Epoch 107/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0348 - accuracy: 0.9917\n",
      "Epoch 108/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0344 - accuracy: 0.9917\n",
      "Epoch 109/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0340 - accuracy: 0.9917\n",
      "Epoch 110/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0336 - accuracy: 0.9917\n",
      "Epoch 111/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0332 - accuracy: 0.9917\n",
      "Epoch 112/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0328 - accuracy: 0.9917\n",
      "Epoch 113/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0324 - accuracy: 0.9917\n",
      "Epoch 114/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0320 - accuracy: 0.9917\n",
      "Epoch 115/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0316 - accuracy: 0.9917\n",
      "Epoch 116/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0312 - accuracy: 0.9917\n",
      "Epoch 117/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0311 - accuracy: 0.9917\n",
      "Epoch 118/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0305 - accuracy: 0.9917\n",
      "Epoch 119/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0302 - accuracy: 0.9917\n",
      "Epoch 120/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0298 - accuracy: 0.9917\n",
      "Epoch 121/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0294 - accuracy: 0.9917\n",
      "Epoch 122/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0290 - accuracy: 0.9917\n",
      "Epoch 123/125\n",
      "483/483 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.99 - 0s 16us/step - loss: 0.0287 - accuracy: 0.9917\n",
      "Epoch 124/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0284 - accuracy: 0.9917\n",
      "Epoch 125/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0280 - accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x280c790c588>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 100, epochs = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"model_with_dropout_layer.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the results of Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  1]\n",
      " [ 0 36]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the confsuion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot = True)\n",
    "\n",
    "# annotbool or rectangular dataset, optional\n",
    "# If True, write the data value in each cell. If an array-like with the same shape as data, then use this to annotate the heatmap instead of the data.\n",
    "\n",
    "plt.savefig(\"confusion_matrix_dropout.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
