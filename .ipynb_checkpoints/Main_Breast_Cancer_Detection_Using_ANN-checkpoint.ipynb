{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 2:-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'M' 'M'\n",
      " 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'M' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'B']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d92b148d08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y, label = \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.350e+01 1.271e+01 8.569e+01 ... 2.210e-02 2.267e-01 6.192e-02]\n",
      " [8.196e+00 1.684e+01 5.171e+01 ... 2.564e-02 3.105e-01 7.409e-02]\n",
      " [1.719e+01 2.207e+01 1.116e+02 ... 1.984e-01 3.216e-01 7.570e-02]\n",
      " ...\n",
      " [9.436e+00 1.832e+01 5.982e+01 ... 5.052e-02 2.454e-01 8.136e-02]\n",
      " [9.720e+00 1.822e+01 6.073e+01 ... 0.000e+00 1.909e-01 6.559e-02]\n",
      " [1.151e+01 2.393e+01 7.452e+01 ... 9.653e-02 2.112e-01 8.732e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.340e+01 2.052e+01 8.864e+01 ... 2.051e-01 3.585e-01 1.109e-01]\n",
      " [1.321e+01 2.525e+01 8.410e+01 ... 6.005e-02 2.444e-01 6.788e-02]\n",
      " [1.402e+01 1.566e+01 8.959e+01 ... 8.216e-02 2.136e-01 6.710e-02]\n",
      " ...\n",
      " [1.243e+01 1.700e+01 7.860e+01 ... 2.832e-02 1.901e-01 5.932e-02]\n",
      " [1.603e+01 1.551e+01 1.058e+02 ... 1.981e-01 3.019e-01 9.124e-02]\n",
      " [1.232e+01 1.239e+01 7.885e+01 ... 9.391e-02 2.827e-01 6.771e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1725321  -1.54666427 -0.25304409 ... -1.41901523 -1.01864958\n",
      "  -1.21862477]\n",
      " [-1.67708411 -0.55788745 -1.65424438 ... -1.3647525   0.32299179\n",
      "  -0.54226125]\n",
      " [ 0.87418677  0.69424397  0.81538145 ...  1.28339165  0.50070324\n",
      "  -0.45278342]\n",
      " ...\n",
      " [-1.32534118 -0.20355581 -1.31982018 ... -0.9833805  -0.71926183\n",
      "  -0.13822158]\n",
      " [-1.2447807  -0.22749713 -1.28229539 ... -1.75777412 -1.59180903\n",
      "  -1.01465976]\n",
      " [-0.73702277  1.13955266 -0.71365054 ... -0.27811821 -1.26680521\n",
      "   0.19301314]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23876518  0.11504267 -0.16915688 ...  1.33496101  1.20351176\n",
      "   1.4405982 ]\n",
      " [-0.29313663  1.08734806 -0.35512227 ... -0.79585962 -0.76301439\n",
      "  -0.90696341]\n",
      " [-0.06134257 -0.88398569 -0.13024341 ... -0.47105821 -1.29385581\n",
      "  -0.94952729]\n",
      " ...\n",
      " [-0.51634572 -0.60853343 -0.58041075 ... -1.26198122 -1.69888092\n",
      "  -1.37407469]\n",
      " [ 0.51385009 -0.9148199   0.53374317 ...  1.23212927  0.22800448\n",
      "   0.36777018]\n",
      " [-0.54782392 -1.55617145 -0.57017036 ... -0.29844778 -0.10290964\n",
      "  -0.91624015]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and first hidden layer\n",
    "classifier.add(Dense(16, activation = \"relu\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(32, activation = \"relu\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "483/483 [==============================] - 0s 264us/step - loss: 0.7559 - accuracy: 0.6377\n",
      "Epoch 2/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.6638 - accuracy: 0.6646\n",
      "Epoch 3/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.5873 - accuracy: 0.7412\n",
      "Epoch 4/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.5188 - accuracy: 0.8323\n",
      "Epoch 5/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.4626 - accuracy: 0.8778\n",
      "Epoch 6/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.4137 - accuracy: 0.9027\n",
      "Epoch 7/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.3709 - accuracy: 0.9193\n",
      "Epoch 8/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.3330 - accuracy: 0.9213\n",
      "Epoch 9/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.3005 - accuracy: 0.9234\n",
      "Epoch 10/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.2732 - accuracy: 0.9317\n",
      "Epoch 11/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.2495 - accuracy: 0.9337\n",
      "Epoch 12/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.2292 - accuracy: 0.9379\n",
      "Epoch 13/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.2118 - accuracy: 0.9441\n",
      "Epoch 14/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1971 - accuracy: 0.9482\n",
      "Epoch 15/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1843 - accuracy: 0.9524\n",
      "Epoch 16/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.1731 - accuracy: 0.9565\n",
      "Epoch 17/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.1635 - accuracy: 0.9565\n",
      "Epoch 18/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1551 - accuracy: 0.9565\n",
      "Epoch 19/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1472 - accuracy: 0.9565\n",
      "Epoch 20/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1404 - accuracy: 0.9565\n",
      "Epoch 21/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1341 - accuracy: 0.9565\n",
      "Epoch 22/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1285 - accuracy: 0.9627\n",
      "Epoch 23/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1233 - accuracy: 0.9627\n",
      "Epoch 24/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1190 - accuracy: 0.9648\n",
      "Epoch 25/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1144 - accuracy: 0.9648\n",
      "Epoch 26/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.1105 - accuracy: 0.9648\n",
      "Epoch 27/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1069 - accuracy: 0.9648\n",
      "Epoch 28/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1036 - accuracy: 0.9669\n",
      "Epoch 29/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.1005 - accuracy: 0.9669\n",
      "Epoch 30/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0975 - accuracy: 0.9710\n",
      "Epoch 31/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0951 - accuracy: 0.9752\n",
      "Epoch 32/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0926 - accuracy: 0.9731\n",
      "Epoch 33/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0904 - accuracy: 0.9731\n",
      "Epoch 34/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0882 - accuracy: 0.9731\n",
      "Epoch 35/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0863 - accuracy: 0.9731\n",
      "Epoch 36/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0845 - accuracy: 0.9752\n",
      "Epoch 37/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0827 - accuracy: 0.9752\n",
      "Epoch 38/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0811 - accuracy: 0.9752\n",
      "Epoch 39/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0795 - accuracy: 0.9772\n",
      "Epoch 40/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0781 - accuracy: 0.9772\n",
      "Epoch 41/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0766 - accuracy: 0.9772\n",
      "Epoch 42/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0754 - accuracy: 0.9772\n",
      "Epoch 43/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0741 - accuracy: 0.9772\n",
      "Epoch 44/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0728 - accuracy: 0.9772\n",
      "Epoch 45/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0717 - accuracy: 0.9793\n",
      "Epoch 46/125\n",
      "483/483 [==============================] - 0s 23us/step - loss: 0.0706 - accuracy: 0.9793\n",
      "Epoch 47/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0696 - accuracy: 0.9793\n",
      "Epoch 48/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0686 - accuracy: 0.9814\n",
      "Epoch 49/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0675 - accuracy: 0.9814\n",
      "Epoch 50/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0666 - accuracy: 0.9814\n",
      "Epoch 51/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0656 - accuracy: 0.9814\n",
      "Epoch 52/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0647 - accuracy: 0.9814\n",
      "Epoch 53/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0637 - accuracy: 0.9814\n",
      "Epoch 54/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0629 - accuracy: 0.9814\n",
      "Epoch 55/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0620 - accuracy: 0.9814\n",
      "Epoch 56/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0613 - accuracy: 0.9814\n",
      "Epoch 57/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0603 - accuracy: 0.9814\n",
      "Epoch 58/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0596 - accuracy: 0.9814\n",
      "Epoch 59/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0589 - accuracy: 0.9834\n",
      "Epoch 60/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0580 - accuracy: 0.9814\n",
      "Epoch 61/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0573 - accuracy: 0.9834\n",
      "Epoch 62/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0565 - accuracy: 0.9834\n",
      "Epoch 63/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0558 - accuracy: 0.9834\n",
      "Epoch 64/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0551 - accuracy: 0.9834\n",
      "Epoch 65/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0543 - accuracy: 0.9834\n",
      "Epoch 66/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0537 - accuracy: 0.9855\n",
      "Epoch 67/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0529 - accuracy: 0.9855\n",
      "Epoch 68/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0522 - accuracy: 0.9876\n",
      "Epoch 69/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0514 - accuracy: 0.9876\n",
      "Epoch 70/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0509 - accuracy: 0.9876\n",
      "Epoch 71/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0502 - accuracy: 0.9876\n",
      "Epoch 72/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0494 - accuracy: 0.9876\n",
      "Epoch 73/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0489 - accuracy: 0.9876\n",
      "Epoch 74/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0483 - accuracy: 0.9876\n",
      "Epoch 75/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0476 - accuracy: 0.9876\n",
      "Epoch 76/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0470 - accuracy: 0.9876\n",
      "Epoch 77/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0464 - accuracy: 0.9876\n",
      "Epoch 78/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0458 - accuracy: 0.9876\n",
      "Epoch 79/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0452 - accuracy: 0.9876\n",
      "Epoch 80/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 0s 17us/step - loss: 0.0446 - accuracy: 0.9876\n",
      "Epoch 81/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0441 - accuracy: 0.9876\n",
      "Epoch 82/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0436 - accuracy: 0.9876\n",
      "Epoch 83/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0430 - accuracy: 0.9876\n",
      "Epoch 84/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0425 - accuracy: 0.9876\n",
      "Epoch 85/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0419 - accuracy: 0.9876\n",
      "Epoch 86/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0414 - accuracy: 0.9876\n",
      "Epoch 87/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0409 - accuracy: 0.9876\n",
      "Epoch 88/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0404 - accuracy: 0.9876\n",
      "Epoch 89/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0398 - accuracy: 0.9876\n",
      "Epoch 90/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0393 - accuracy: 0.9876\n",
      "Epoch 91/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0388 - accuracy: 0.9876\n",
      "Epoch 92/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0383 - accuracy: 0.9876\n",
      "Epoch 93/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0378 - accuracy: 0.9876\n",
      "Epoch 94/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0373 - accuracy: 0.9876\n",
      "Epoch 95/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0368 - accuracy: 0.9876\n",
      "Epoch 96/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0363 - accuracy: 0.9876\n",
      "Epoch 97/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0359 - accuracy: 0.9876\n",
      "Epoch 98/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0354 - accuracy: 0.9876\n",
      "Epoch 99/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0348 - accuracy: 0.9876\n",
      "Epoch 100/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0344 - accuracy: 0.9876\n",
      "Epoch 101/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0339 - accuracy: 0.9876\n",
      "Epoch 102/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0335 - accuracy: 0.9876\n",
      "Epoch 103/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0330 - accuracy: 0.9876\n",
      "Epoch 104/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0326 - accuracy: 0.9876\n",
      "Epoch 105/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0320 - accuracy: 0.9876\n",
      "Epoch 106/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0316 - accuracy: 0.9876\n",
      "Epoch 107/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0313 - accuracy: 0.9876\n",
      "Epoch 108/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0306 - accuracy: 0.9876\n",
      "Epoch 109/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0302 - accuracy: 0.9876\n",
      "Epoch 110/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0298 - accuracy: 0.9896\n",
      "Epoch 111/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0293 - accuracy: 0.9896\n",
      "Epoch 112/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0289 - accuracy: 0.9896\n",
      "Epoch 113/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0284 - accuracy: 0.9896\n",
      "Epoch 114/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0280 - accuracy: 0.9896\n",
      "Epoch 115/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0276 - accuracy: 0.9896\n",
      "Epoch 116/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0272 - accuracy: 0.9917\n",
      "Epoch 117/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0268 - accuracy: 0.9917\n",
      "Epoch 118/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0263 - accuracy: 0.9917\n",
      "Epoch 119/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0260 - accuracy: 0.9917\n",
      "Epoch 120/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0257 - accuracy: 0.9917\n",
      "Epoch 121/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0252 - accuracy: 0.9917\n",
      "Epoch 122/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0249 - accuracy: 0.9917\n",
      "Epoch 123/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0246 - accuracy: 0.9917\n",
      "Epoch 124/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0242 - accuracy: 0.9938\n",
      "Epoch 125/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0239 - accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d930d74148>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 100, epochs = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the results of Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  2]\n",
      " [ 0 36]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the confsuion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot = True)\n",
    "\n",
    "# annotbool or rectangular dataset, optional\n",
    "# If True, write the data value in each cell. If an array-like with the same shape as data, then use this to annotate the heatmap instead of the data.\n",
    "\n",
    "plt.savefig(\"confusion_matrix.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
