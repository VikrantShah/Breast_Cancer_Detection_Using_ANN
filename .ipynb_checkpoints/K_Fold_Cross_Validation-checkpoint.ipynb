{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 2:-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B'\n",
      " 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'M' 'M'\n",
      " 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'M' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M'\n",
      " 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'M' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'M' 'B']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y, label = \"Diagnosis\")\n",
    "plt.savefig(\"Diagnosis.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.350e+01 1.271e+01 8.569e+01 ... 2.210e-02 2.267e-01 6.192e-02]\n",
      " [8.196e+00 1.684e+01 5.171e+01 ... 2.564e-02 3.105e-01 7.409e-02]\n",
      " [1.719e+01 2.207e+01 1.116e+02 ... 1.984e-01 3.216e-01 7.570e-02]\n",
      " ...\n",
      " [9.436e+00 1.832e+01 5.982e+01 ... 5.052e-02 2.454e-01 8.136e-02]\n",
      " [9.720e+00 1.822e+01 6.073e+01 ... 0.000e+00 1.909e-01 6.559e-02]\n",
      " [1.151e+01 2.393e+01 7.452e+01 ... 9.653e-02 2.112e-01 8.732e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.340e+01 2.052e+01 8.864e+01 ... 2.051e-01 3.585e-01 1.109e-01]\n",
      " [1.321e+01 2.525e+01 8.410e+01 ... 6.005e-02 2.444e-01 6.788e-02]\n",
      " [1.402e+01 1.566e+01 8.959e+01 ... 8.216e-02 2.136e-01 6.710e-02]\n",
      " ...\n",
      " [1.243e+01 1.700e+01 7.860e+01 ... 2.832e-02 1.901e-01 5.932e-02]\n",
      " [1.603e+01 1.551e+01 1.058e+02 ... 1.981e-01 3.019e-01 9.124e-02]\n",
      " [1.232e+01 1.239e+01 7.885e+01 ... 9.391e-02 2.827e-01 6.771e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1725321  -1.54666427 -0.25304409 ... -1.41901523 -1.01864958\n",
      "  -1.21862477]\n",
      " [-1.67708411 -0.55788745 -1.65424438 ... -1.3647525   0.32299179\n",
      "  -0.54226125]\n",
      " [ 0.87418677  0.69424397  0.81538145 ...  1.28339165  0.50070324\n",
      "  -0.45278342]\n",
      " ...\n",
      " [-1.32534118 -0.20355581 -1.31982018 ... -0.9833805  -0.71926183\n",
      "  -0.13822158]\n",
      " [-1.2447807  -0.22749713 -1.28229539 ... -1.75777412 -1.59180903\n",
      "  -1.01465976]\n",
      " [-0.73702277  1.13955266 -0.71365054 ... -0.27811821 -1.26680521\n",
      "   0.19301314]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20089847  0.32315339 -0.1313978  ...  1.38609231  1.09147372\n",
      "   1.50350219]\n",
      " [-0.25479456  1.45557818 -0.31860937 ... -0.83730031 -0.73527187\n",
      "  -0.88739005]\n",
      " [-0.025027   -0.84039511 -0.09222357 ... -0.49838813 -1.22838111\n",
      "  -0.93073956]\n",
      " ...\n",
      " [-0.47605221 -0.51958133 -0.54540754 ... -1.32367223 -1.60461706\n",
      "  -1.36312314]\n",
      " [ 0.54513693 -0.8763071   0.57621247 ...  1.27879312  0.18530544\n",
      "   0.41087222]\n",
      " [-0.50725521 -1.62327652 -0.53509853 ... -0.31827877 -0.12208733\n",
      "  -0.89683802]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and first hidden layer\n",
    "classifier.add(Dense(16, activation = \"relu\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(32, activation = \"relu\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "483/483 [==============================] - 0s 254us/step - loss: 0.8059 - accuracy: 0.3665\n",
      "Epoch 2/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.7026 - accuracy: 0.5652\n",
      "Epoch 3/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.6136 - accuracy: 0.6812\n",
      "Epoch 4/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.5380 - accuracy: 0.7930\n",
      "Epoch 5/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.4696 - accuracy: 0.9006\n",
      "Epoch 6/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.4112 - accuracy: 0.9317\n",
      "Epoch 7/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.3620 - accuracy: 0.9317\n",
      "Epoch 8/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.3181 - accuracy: 0.9379\n",
      "Epoch 9/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.2816 - accuracy: 0.9441\n",
      "Epoch 10/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.2516 - accuracy: 0.9482\n",
      "Epoch 11/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.2263 - accuracy: 0.9524\n",
      "Epoch 12/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.2055 - accuracy: 0.9565\n",
      "Epoch 13/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1879 - accuracy: 0.9607\n",
      "Epoch 14/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1730 - accuracy: 0.9607\n",
      "Epoch 15/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1605 - accuracy: 0.9627\n",
      "Epoch 16/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1496 - accuracy: 0.9669\n",
      "Epoch 17/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1407 - accuracy: 0.9648\n",
      "Epoch 18/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1326 - accuracy: 0.9648\n",
      "Epoch 19/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1256 - accuracy: 0.9648\n",
      "Epoch 20/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.1197 - accuracy: 0.9648\n",
      "Epoch 21/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.1142 - accuracy: 0.9648\n",
      "Epoch 22/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.1095 - accuracy: 0.9669\n",
      "Epoch 23/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.1054 - accuracy: 0.9689\n",
      "Epoch 24/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.1016 - accuracy: 0.9689\n",
      "Epoch 25/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0981 - accuracy: 0.9689\n",
      "Epoch 26/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0951 - accuracy: 0.9689\n",
      "Epoch 27/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0926 - accuracy: 0.9689\n",
      "Epoch 28/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0898 - accuracy: 0.9689\n",
      "Epoch 29/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0876 - accuracy: 0.9689\n",
      "Epoch 30/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0856 - accuracy: 0.9710\n",
      "Epoch 31/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0833 - accuracy: 0.9710\n",
      "Epoch 32/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0814 - accuracy: 0.9731\n",
      "Epoch 33/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0797 - accuracy: 0.9731\n",
      "Epoch 34/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0780 - accuracy: 0.9731\n",
      "Epoch 35/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0764 - accuracy: 0.9752\n",
      "Epoch 36/125\n",
      "483/483 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - 0s 25us/step - loss: 0.0749 - accuracy: 0.9772\n",
      "Epoch 37/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0735 - accuracy: 0.9772\n",
      "Epoch 38/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0720 - accuracy: 0.9772\n",
      "Epoch 39/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0708 - accuracy: 0.9793\n",
      "Epoch 40/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0695 - accuracy: 0.9793\n",
      "Epoch 41/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0682 - accuracy: 0.9793\n",
      "Epoch 42/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0670 - accuracy: 0.9793\n",
      "Epoch 43/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0660 - accuracy: 0.9793\n",
      "Epoch 44/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0649 - accuracy: 0.9814\n",
      "Epoch 45/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0637 - accuracy: 0.9814\n",
      "Epoch 46/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0628 - accuracy: 0.9814\n",
      "Epoch 47/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0618 - accuracy: 0.9814\n",
      "Epoch 48/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0608 - accuracy: 0.9834\n",
      "Epoch 49/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0599 - accuracy: 0.9834\n",
      "Epoch 50/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0591 - accuracy: 0.9834\n",
      "Epoch 51/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0582 - accuracy: 0.9834\n",
      "Epoch 52/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0573 - accuracy: 0.9834\n",
      "Epoch 53/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0565 - accuracy: 0.9834\n",
      "Epoch 54/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0557 - accuracy: 0.9834\n",
      "Epoch 55/125\n",
      "483/483 [==============================] - 0s 21us/step - loss: 0.0549 - accuracy: 0.9834\n",
      "Epoch 56/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0541 - accuracy: 0.9834\n",
      "Epoch 57/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0532 - accuracy: 0.9834\n",
      "Epoch 58/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0525 - accuracy: 0.9834\n",
      "Epoch 59/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0516 - accuracy: 0.9834\n",
      "Epoch 60/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0509 - accuracy: 0.9834\n",
      "Epoch 61/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0504 - accuracy: 0.9855\n",
      "Epoch 62/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0494 - accuracy: 0.9876\n",
      "Epoch 63/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0487 - accuracy: 0.9876\n",
      "Epoch 64/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0480 - accuracy: 0.9876\n",
      "Epoch 65/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0474 - accuracy: 0.9876\n",
      "Epoch 66/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0467 - accuracy: 0.9876\n",
      "Epoch 67/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0460 - accuracy: 0.9876\n",
      "Epoch 68/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0455 - accuracy: 0.9876\n",
      "Epoch 69/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0448 - accuracy: 0.9876\n",
      "Epoch 70/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0442 - accuracy: 0.9876\n",
      "Epoch 71/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0436 - accuracy: 0.9876\n",
      "Epoch 72/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0430 - accuracy: 0.9876\n",
      "Epoch 73/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0424 - accuracy: 0.9876\n",
      "Epoch 74/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0419 - accuracy: 0.9876\n",
      "Epoch 75/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0413 - accuracy: 0.9876\n",
      "Epoch 76/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0408 - accuracy: 0.9876\n",
      "Epoch 77/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0402 - accuracy: 0.9876\n",
      "Epoch 78/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0396 - accuracy: 0.9876\n",
      "Epoch 79/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0391 - accuracy: 0.9876\n",
      "Epoch 80/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 0s 14us/step - loss: 0.0386 - accuracy: 0.9876\n",
      "Epoch 81/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0381 - accuracy: 0.9896\n",
      "Epoch 82/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0377 - accuracy: 0.9876\n",
      "Epoch 83/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0371 - accuracy: 0.9896\n",
      "Epoch 84/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0366 - accuracy: 0.9917\n",
      "Epoch 85/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0361 - accuracy: 0.9917\n",
      "Epoch 86/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0356 - accuracy: 0.9917\n",
      "Epoch 87/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0351 - accuracy: 0.9917\n",
      "Epoch 88/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0345 - accuracy: 0.9917\n",
      "Epoch 89/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0341 - accuracy: 0.9917\n",
      "Epoch 90/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0336 - accuracy: 0.9917\n",
      "Epoch 91/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0331 - accuracy: 0.9917\n",
      "Epoch 92/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0325 - accuracy: 0.9917\n",
      "Epoch 93/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0320 - accuracy: 0.9917\n",
      "Epoch 94/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0315 - accuracy: 0.9917\n",
      "Epoch 95/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0312 - accuracy: 0.9917\n",
      "Epoch 96/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0307 - accuracy: 0.9917\n",
      "Epoch 97/125\n",
      "483/483 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 1.00 - 0s 17us/step - loss: 0.0302 - accuracy: 0.9917\n",
      "Epoch 98/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0298 - accuracy: 0.9917\n",
      "Epoch 99/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0295 - accuracy: 0.9917\n",
      "Epoch 100/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0290 - accuracy: 0.9917\n",
      "Epoch 101/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0286 - accuracy: 0.9917\n",
      "Epoch 102/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0283 - accuracy: 0.9917\n",
      "Epoch 103/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0276 - accuracy: 0.9917\n",
      "Epoch 104/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0273 - accuracy: 0.9917\n",
      "Epoch 105/125\n",
      "483/483 [==============================] - 0s 12us/step - loss: 0.0268 - accuracy: 0.9917\n",
      "Epoch 106/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0265 - accuracy: 0.9917\n",
      "Epoch 107/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0260 - accuracy: 0.9917\n",
      "Epoch 108/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0256 - accuracy: 0.9917\n",
      "Epoch 109/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0253 - accuracy: 0.9917\n",
      "Epoch 110/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0248 - accuracy: 0.9917\n",
      "Epoch 111/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0244 - accuracy: 0.9917\n",
      "Epoch 112/125\n",
      "483/483 [==============================] - 0s 19us/step - loss: 0.0240 - accuracy: 0.9917\n",
      "Epoch 113/125\n",
      "483/483 [==============================] - 0s 15us/step - loss: 0.0236 - accuracy: 0.9917\n",
      "Epoch 114/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0232 - accuracy: 0.9917\n",
      "Epoch 115/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0228 - accuracy: 0.9917\n",
      "Epoch 116/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0226 - accuracy: 0.9917\n",
      "Epoch 117/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0221 - accuracy: 0.9917\n",
      "Epoch 118/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0218 - accuracy: 0.9917\n",
      "Epoch 119/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0215 - accuracy: 0.9917\n",
      "Epoch 120/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0211 - accuracy: 0.9917\n",
      "Epoch 121/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0208 - accuracy: 0.9917\n",
      "Epoch 122/125\n",
      "483/483 [==============================] - 0s 17us/step - loss: 0.0204 - accuracy: 0.9917\n",
      "Epoch 123/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0201 - accuracy: 0.9917\n",
      "Epoch 124/125\n",
      "483/483 [==============================] - 0s 16us/step - loss: 0.0198 - accuracy: 0.9917\n",
      "Epoch 125/125\n",
      "483/483 [==============================] - 0s 14us/step - loss: 0.0195 - accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cf21b0b888>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 100, epochs = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the results of Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0]\n",
      " [ 0 36]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the confsuion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot = True)\n",
    "\n",
    "# annotbool or rectangular dataset, optional\n",
    "# If True, write the data value in each cell. If an array-like with the same shape as data, then use this to annotate the heatmap instead of the data.\n",
    "\n",
    "plt.savefig(\"confusion_matrix.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
